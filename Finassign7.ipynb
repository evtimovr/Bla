{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finassign.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evtimovr/Bla/blob/master/Finassign7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mHoCVALzwae",
        "colab_type": "code",
        "outputId": "5191bfab-d436-49e1-f304-803a3b413f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I45AahO2Jmg8",
        "colab_type": "text"
      },
      "source": [
        "For the purposes of the task, I have used Google Colab, as using its GPU should improve the computional time. An easy way of importing data there is mounting your Google Drive and then you are able to pull everything that is inside of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WFksjXSKNS-",
        "colab_type": "text"
      },
      "source": [
        "First, we need to import relevant packages, that will be used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRrW8qi2XQHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image # library 'pillow' for image processing\n",
        "import os, os.path\n",
        "import glob\n",
        "from os.path import basename\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaONcaPyKUJ4",
        "colab_type": "text"
      },
      "source": [
        "Creating ImageDataGenerator\n",
        "\n",
        "In the case with the photoshopped data there are several ways to import the data in Python. The one used here: ImageDataGenerator allows us to augment the pictures - to change them in a certain way and create additional leaning material for the model this way. The reason we are doing this is to generalize better:  If we zoom some images a bit or turn them around, this would prevent the model from overfitting easily as it won't learn the location of certain pixels for example.\n",
        "\n",
        "Depending on the exact task, there are several functions that could be used: rotating, zooming, changing the shear intensity and brightness.  Of course, it is not a good to make the DataGenerator to create images that don't exist at all in the real data you would like to predict as this would only make it more inaccurate without any special benefit. \n",
        "\n",
        "In this case I am using only few of the options. For example a small rotation would make sense as not all of the faces on the images have face that are in the exact same rotation - zooming in and getting rid of a part of the picture also makes sense as in our dataset most of the revelant information could be found in the pictures in the middle of the image. Additionally, we are rescaling to a range between 0 and 1. This would make the computation easier and will keep the information we need about the differences between different pixels. \n",
        "\n",
        "I use this moment to indroduce some other variables: \n",
        "\n",
        "**dim** - the size of the pictures. In order to make it easier to work with, we are resizing the picture. A rule of thumb is the square root of the original size. \n",
        "\n",
        "**train_batch_size** - how many images will go through the model together. Again important for the quality of learning and the computational time: A higher size of the batch leads to faster calulation but the model could more easily overfit. On the other hand, setting it to \"1\" would lead to adjusment of the parameters for each image which would increase the computational time eanourmously. \n",
        "\n",
        "**input_shape** - specifiying the shape of the data flowing into the sequencial model. The first two are the height and width of the image and \"3\" stays for the 3 channels - red, green, blue as the images are color-images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nMSgHTsxRrt",
        "colab_type": "code",
        "outputId": "73babf5c-336e-4c2f-ac2c-c72ba522b46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "sqrt_dim=60\n",
        "train_batch_size=40\n",
        "input_shape=(sqrt_dim, sqrt_dim, 3)\n",
        "\n",
        "train_generator = ImageDataGenerator(rescale = 1./255, \n",
        "                                   shear_range = 0.1, \n",
        "                                   zoom_range = 0.1,\n",
        "                                   horizontal_flip = False, \n",
        "                                   rotation_range = 20) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8aI95OKVNP8",
        "colab_type": "text"
      },
      "source": [
        "Additionally we have to create a DataGenerator for the test data in order to import this data as well. Here, we don't want to change anything in the images, we are only rescaling them in order to fit the values in the model. Otherwise the data should be as close as possible as the real data, that the model will try to predict. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKjC28t80QXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "test_generator = ImageDataGenerator(rescale = 1./255) # only rescaling with fixed parameters is valid, otherwise do not touch the test set! \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qVAFRndWxD9",
        "colab_type": "text"
      },
      "source": [
        "The actual import of the images happens with the \"flow_from_directory\" function. This way we tell the function where to find the data and it automatically takes the name of the directory where it finds files as labels. We use it as well to resize the images and tell it what batch size should be used during the training. \n",
        "\n",
        "The output shows us how many images were found in the respective folder and their labels - \"fake\" and \"real\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9VuJcgN3oBq",
        "colab_type": "code",
        "outputId": "e1c4214a-d441-41a5-e421-2352ab8cf598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "trainset = train_generator.flow_from_directory('/content/drive/My Drive/Final assignment/train', # specify the directory (you can right click on the folder you need and copy the path)\n",
        "                                                 target_size = (sqrt_dim, sqrt_dim), # you need to check your data,\n",
        "                                                 batch_size = train_batch_size, # batch size = 5 here just for show!\n",
        "                                                 class_mode= 'categorical')\n",
        "\n",
        "\n",
        "validationset = test_generator.flow_from_directory('/content/drive/My Drive/Final assignment/validation', # specify the directory (you can right click on the folder you need and copy the path)\n",
        "                                                 target_size = (sqrt_dim, sqrt_dim), # you need to check your data,\n",
        "                                                 batch_size = train_batch_size, # batch size = 5 here just for show!\n",
        "                                                 class_mode= 'categorical')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 725 images belonging to 2 classes.\n",
            "Found 200 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR4fptK0oDz",
        "colab_type": "code",
        "outputId": "656e091c-48eb-4d34-9f1d-02db3117c0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainset.class_indices"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fake': 0, 'real': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtiG34YWoWDx",
        "colab_type": "text"
      },
      "source": [
        "We also define some variables we would need to use later: number of samples and number of classes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Q4WlGad2oDP",
        "colab_type": "code",
        "outputId": "35d28b92-1af4-4143-d432-ef049d0e5a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_train_samples = len(trainset.filenames)\n",
        "n_classes = len(trainset.class_indices)\n",
        "print(n_train_samples, n_classes)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "725 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRDCNKYy4KPw",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional neural networks implementation**\n",
        "\n",
        "After the data is imported and formated in a suitable way, we are ready to create the model, compile it and then train it.\n",
        "\n",
        "The obvious choice for image classification is the CNN (Convolutional neural network). It has been widely used in the recent years exactly for this type of tasks. \n",
        "The CNN works well on images because it tries to find deep pattern in the images using different filters.\n",
        "\n",
        "Briefly covering the basic structure of the convolutional part of the neural network: \n",
        "\n",
        "- Input --> Convolution --> Pooling --> Dropout-->  Convolution --> Pooling --> Dropout --> Convolution --> Pooling --> Dropout --> Flatten --> Dense --> Dense--> Output \n",
        "\n",
        "The convolutional part consisits of multiple time doing the same things: Convolution--> Pooling --> Dropout\n",
        "- In the \"Convolution\" part we apply filters to the images and create multiple different images focusing on specific features - this way we hope the model will find a certain pattern typical only for the photoshopped images. \n",
        "- After creating those \"new\" images, we cut some of the information out to reduce dimensionallity - in this case I am using MaxPooling and this is the most used pooling algorithm. It basically takes the most activated pixel from a group of pixels and creates a \"new\" image that is a good represantation of the previous one, but consisting of less data. \n",
        "- Then comes the \"Dropout\" layer - this is used to prevent the model from overfitting. It makes the model randomly miss some of the neurons, in this case images. \n",
        "Important to note is that the \"Dropout\" functions rather differently in the convolutional layers and in the fully connected \"Dense\" layers.  What we give as input is the probability for each layer that it gets dropped out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyG4wEx35EGK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "4dd9aba9-c6cd-430a-c022-4a60238456ff"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Dropout # converting pooled feature maps into a large feature vector to prepare for input into the Dense layer\n",
        "from keras.layers import Conv2D # first convolution step, 3D for videos\n",
        "from keras.layers import MaxPooling2D #pooling, downsampling\n",
        "\n",
        "model = Sequential() #initialize\n",
        "model.add(Conv2D(32, #number of filters/feature detectors, number of feature maps you will end up with. Starting with 32 is good practice, grow it if needed and if you have enough computing power\n",
        "                 kernel_size=(3, 3), # size of filters (rows,columns), (3,3) is also a popular choice, it's a hyperparameter to choose\n",
        "                 activation='relu', # removes the negative pixels, introducing non-linearity, \n",
        "                 #we are skipping \"border mode\" - it is set to \"same\" by default\n",
        "                 # strides=(2,2) how the window shifts by in each of the dimensions. Default stride is 1 but we will go for 2 for faster implementation by reducing the image size similar to pooling\n",
        "                 input_shape=input_shape)) # (3 if color/1 if b&w, height, width), all images have to come in a single format\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), #the size of the pooled feature map\n",
        "                      strides=None)) # default: pool_size, here: (2,2)\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu',padding = 'same'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.6))\n",
        "\n",
        "\n",
        "model.add(Conv2D(16, (1, 1), activation='relu'))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu')) # that's our classifier bit, th size is up to you, choose wisely taking the size of picture in mind (smth between nuber of input and number of output nodes could be another rule of thumb)\n",
        "model.add(Dropout(0.6))\n",
        "model.add(Dense(n_classes, activation='softmax')) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0812 21:41:16.787194 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0812 21:41:16.811980 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0812 21:41:16.816143 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0812 21:41:16.842851 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0812 21:41:16.871249 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0812 21:41:16.887344 139845420373888 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0812 21:41:16.942271 139845420373888 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0812 21:41:16.987756 139845420373888 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0812 21:41:17.051084 139845420373888 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5Y1k6GZz1mV",
        "colab_type": "text"
      },
      "source": [
        "After creating the model and shaping its structure comes the time for the model to be compiled - we have to tell it how to learn, how to optimize and choose which direction to move the coefficient in and tell it how to calculate the loss. \n",
        "\n",
        "After this it is useful to display the summary of the model to have an overview and understand better what is happening. The \"Ouput Shape\" column gives us information about how the information flowing from one to the next layer would look like, while the parameter column shows us how many variables should be learned and adjusted by the model. \n",
        "\n",
        "It is fairly important to understand what the model actually changes in each run using backpropagation. In the convolutional layers the model is trying to find the filters to be used - it adjusts them and checks which one gives the lowest loss compared to the real data. In the \"Dense\" layers it adjusts the weights of each neuron. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCf7bkFA7BV2",
        "colab_type": "code",
        "outputId": "1812759b-24f6-4ccd-a5fe-3c4a52989179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0812 21:41:20.874883 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0812 21:41:20.910254 139845420373888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 58, 58, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 29, 29, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 27, 27, 128)       36992     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 13, 13, 128)       147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 6, 6, 16)          2064      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               147712    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 514       \n",
            "=================================================================\n",
            "Total params: 335,762\n",
            "Trainable params: 335,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O58l84h7qpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_epochs = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5K0VjYzDeB",
        "colab_type": "text"
      },
      "source": [
        "To cope with the overfitting issue we also create a callback that should stop the training of the model when it reaches certain accuracy. It checks the validation accuracy and then based on a condition decides if the training should proceed or stop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjwDmfMd5lQl",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaZiMychAlre",
        "colab_type": "code",
        "outputId": "871eb77f-b892-470d-b908-06528a096f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "class AccuracyCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Callback that stops training on requested accuracy.\"\"\"\n",
        "    def __init__(self, target_accuracy=0.997):\n",
        "        self.target_accuracy = target_accuracy\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        if (logs.get('val_acc') >= self.target_accuracy):\n",
        "            print(\"Target validation accuracy (%f) reached !\" % self.target_accuracy )\n",
        "            self.model.stop_training = True\n",
        "            \n",
        "target_accuracy = 0.579\n",
        "max_epochs = 40\n",
        "print(\"\\nTraining untill accuracy reaches %s or for %i epochs\" %(target_accuracy, max_epochs))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training untill accuracy reaches 0.579 or for 40 epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP6_4QEGWPW2",
        "colab_type": "text"
      },
      "source": [
        "We know that the steps_per_epoch*batch_size = number of images going through the model. In the same time we wanna help the model generalize (this why we are using a generator to not only use the original images) and create a learnable model (too many variations of the original data makes it hard and slow for the model to learn at all)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1fdmX1K7iJ4",
        "colab_type": "code",
        "outputId": "8abfa80d-bc22-4f3c-9cac-53dd6ba3b166",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "source": [
        "history=model.fit_generator(trainset,\n",
        "                    steps_per_epoch= 400,# number of times generator will be used,that's how we control for the number of generated images, overall size of set divided by the batch size to make sure everything gets used\n",
        "                    validation_data=validationset,\n",
        "                    validation_steps= 10, # 340/2\n",
        "                    epochs=max_epochs,\n",
        "                    callbacks=[AccuracyCallback(target_accuracy)]\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0812 21:41:35.226103 139845420373888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "400/400 [==============================] - 230s 575ms/step - loss: 0.6951 - acc: 0.5029 - val_loss: 0.6900 - val_acc: 0.5750\n",
            "Epoch 2/40\n",
            "400/400 [==============================] - 226s 566ms/step - loss: 0.6836 - acc: 0.5624 - val_loss: 0.6738 - val_acc: 0.6150\n",
            "Target validation accuracy (0.579000) reached !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22pmffGG7okO",
        "colab_type": "text"
      },
      "source": [
        "After training the model it is important to save it, because running it again, despite doing it with the exact same parameters, it won't give the same results as the generator will feed it with different pictures every time it runs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTkqNxUT5Gfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model9')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CavfqV1S8lXh",
        "colab_type": "text"
      },
      "source": [
        "The whole point of creating a model is for it to predict data for which we don't know the labels. We import them again with the test_generator.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYRhBJwXFBzS",
        "colab_type": "code",
        "outputId": "735efdf7-f5ce-466e-a21a-10fe739dbd8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "unknownset = test_generator.flow_from_directory('/content/drive/My Drive/Final assignment/unknown', # specify the directory (you can right click on the folder you need and copy the path)\n",
        "                                                 target_size = (sqrt_dim, sqrt_dim), # you need to check your data,\n",
        "                                                 batch_size = train_batch_size, # batch size = 5 here just for show!\n",
        "                                                 class_mode= 'categorical')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 481 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITrwwnwsTARr",
        "colab_type": "code",
        "outputId": "e5fda0a4-d93f-4124-f91c-6d14062ac139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#You have to reset or reassign the test generator\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('/content/drive/My Drive/Final assignment/unknown/',  # specify the directory\n",
        "                                            target_size = (sqrt_dim,sqrt_dim),\n",
        "                                            batch_size = 1, #to sample an image just once\n",
        "                                            class_mode = 'categorical',\n",
        "                                            shuffle = False)\n",
        "filenames = test_generator.filenames\n",
        "nb_samples = len(filenames)\n",
        "\n",
        "yhat = model.predict_generator(test_generator,steps = nb_samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 481 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oVnKm6PdHoC",
        "colab_type": "code",
        "outputId": "fbbba6b3-4803-4790-f974-074b698a0a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filenames[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'unknown/003358d7a7461d8cdbe9.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_dAmAkdbOTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "names = []\n",
        "predictions=[]\n",
        "\n",
        "for i in range(0,len(filenames)):\n",
        "     names.append(filenames[i])\n",
        "\n",
        "for i in range(0,len(filenames)):\n",
        "     predictions.append(yhat[i,0])\n",
        "    \n",
        "pred = pd.DataFrame({'ID':names,'fake':predictions})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MWZSXZMhudu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred.to_csv(\"predictions7.csv\",header = True,index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwEE9M2UhkPY",
        "colab_type": "code",
        "outputId": "b3e0d162-66ba-454c-baa9-088b38645d30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>unknown/00217efccd6f697eb937.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>unknown/003358d7a7461d8cdbe9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>unknown/008cae7827beecd1aab3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>unknown/00b75601a2272c9c8f3e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>unknown/01224275d3e25f62a920.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>unknown/01a1a6b856bffedc304c.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>unknown/020c00becfcd4e7938af.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>unknown/045bcecab3b5f0f68836.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>unknown/046d3ccaf5e4888cff8f.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>unknown/056907b340072139457a.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>unknown/0616140ccf8da205dae1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>unknown/06bd053d5cd7cb4ecfa0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>unknown/0847ce848979b5071266.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>unknown/08a9b88ec64f9d4a7ea2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>unknown/08d407136658f90ca09b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>unknown/091a43a48d1fef10d077.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>unknown/09389d45c8f600eda5e1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>unknown/09951d4f497fbdce0724.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>unknown/0a48e27b6e155160fe13.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>unknown/0aca4f842d0cdd5e2220.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>unknown/0ae51c69b77794df5bf8.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>unknown/0b40cdb00b43ea106e79.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>unknown/0b7362925433b426b994.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>unknown/0bdda3be627302b14edc.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>unknown/0c03ef95d59620ea36ed.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>unknown/0c36bb35533789e99c2e.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>unknown/0cf65c5771d3bbd394cc.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>unknown/0cfe523c770b7ec905dd.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>unknown/0d204ff575d2690f35e3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>unknown/0d36dd162e2aed48c972.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>unknown/efd8e62efc02812664be.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>unknown/f03de53b8026cb81fb1a.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>unknown/f09811ad3ae292a07e81.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>unknown/f1e8bae767aa2a06d2fb.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>unknown/f20dbad0f985fc682d54.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>unknown/f2621e0764597fde65a0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>unknown/f381739f098f962fca67.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>unknown/f39efc4d355beb464afd.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>unknown/f427e4f2c6ed25052984.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>unknown/f467b6000b34533104c6.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>unknown/f4ff1b2db465e95e8af1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>unknown/f537258a9143a591eaa4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>unknown/f5aa72aa77528d69e41d.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>unknown/f60b2caf927164287acc.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>unknown/f69533d76a659f4e7ccf.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>unknown/f7c721742c86ffec64e9.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>unknown/f8112c58b54784000053.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>unknown/f9454d6da262850d6d3a.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>unknown/f95d52ebe32b331ec361.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470</th>\n",
              "      <td>unknown/f991ac31b402528a0fb4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>unknown/fa1127b0a7fcd98af385.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>unknown/fa96c00d587eac218b8b.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>unknown/fab5b6a96ad9e75f48c8.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>unknown/fb4f013fd7ee08a4a9bb.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>unknown/fbecda8e838fcee4dd20.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>unknown/fc1ba171d3a63345f516.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>unknown/fd2807a7befa288f3321.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>unknown/fd5737122300961d1bca.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>unknown/fd79ce851d0af59df9c3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>unknown/fe8fcc2796872d12ff0d.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>481 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  col\n",
              "0    unknown/00217efccd6f697eb937.jpg\n",
              "1    unknown/003358d7a7461d8cdbe9.jpg\n",
              "2    unknown/008cae7827beecd1aab3.jpg\n",
              "3    unknown/00b75601a2272c9c8f3e.jpg\n",
              "4    unknown/01224275d3e25f62a920.jpg\n",
              "5    unknown/01a1a6b856bffedc304c.jpg\n",
              "6    unknown/020c00becfcd4e7938af.jpg\n",
              "7    unknown/045bcecab3b5f0f68836.jpg\n",
              "8    unknown/046d3ccaf5e4888cff8f.jpg\n",
              "9    unknown/056907b340072139457a.jpg\n",
              "10   unknown/0616140ccf8da205dae1.jpg\n",
              "11   unknown/06bd053d5cd7cb4ecfa0.jpg\n",
              "12   unknown/0847ce848979b5071266.jpg\n",
              "13   unknown/08a9b88ec64f9d4a7ea2.jpg\n",
              "14   unknown/08d407136658f90ca09b.jpg\n",
              "15   unknown/091a43a48d1fef10d077.jpg\n",
              "16   unknown/09389d45c8f600eda5e1.jpg\n",
              "17   unknown/09951d4f497fbdce0724.jpg\n",
              "18   unknown/0a48e27b6e155160fe13.jpg\n",
              "19   unknown/0aca4f842d0cdd5e2220.jpg\n",
              "20   unknown/0ae51c69b77794df5bf8.jpg\n",
              "21   unknown/0b40cdb00b43ea106e79.jpg\n",
              "22   unknown/0b7362925433b426b994.jpg\n",
              "23   unknown/0bdda3be627302b14edc.jpg\n",
              "24   unknown/0c03ef95d59620ea36ed.jpg\n",
              "25   unknown/0c36bb35533789e99c2e.jpg\n",
              "26   unknown/0cf65c5771d3bbd394cc.jpg\n",
              "27   unknown/0cfe523c770b7ec905dd.jpg\n",
              "28   unknown/0d204ff575d2690f35e3.jpg\n",
              "29   unknown/0d36dd162e2aed48c972.jpg\n",
              "..                                ...\n",
              "451  unknown/efd8e62efc02812664be.jpg\n",
              "452  unknown/f03de53b8026cb81fb1a.jpg\n",
              "453  unknown/f09811ad3ae292a07e81.jpg\n",
              "454  unknown/f1e8bae767aa2a06d2fb.jpg\n",
              "455  unknown/f20dbad0f985fc682d54.jpg\n",
              "456  unknown/f2621e0764597fde65a0.jpg\n",
              "457  unknown/f381739f098f962fca67.jpg\n",
              "458  unknown/f39efc4d355beb464afd.jpg\n",
              "459  unknown/f427e4f2c6ed25052984.jpg\n",
              "460  unknown/f467b6000b34533104c6.jpg\n",
              "461  unknown/f4ff1b2db465e95e8af1.jpg\n",
              "462  unknown/f537258a9143a591eaa4.jpg\n",
              "463  unknown/f5aa72aa77528d69e41d.jpg\n",
              "464  unknown/f60b2caf927164287acc.jpg\n",
              "465  unknown/f69533d76a659f4e7ccf.jpg\n",
              "466  unknown/f7c721742c86ffec64e9.jpg\n",
              "467  unknown/f8112c58b54784000053.jpg\n",
              "468  unknown/f9454d6da262850d6d3a.jpg\n",
              "469  unknown/f95d52ebe32b331ec361.jpg\n",
              "470  unknown/f991ac31b402528a0fb4.jpg\n",
              "471  unknown/fa1127b0a7fcd98af385.jpg\n",
              "472  unknown/fa96c00d587eac218b8b.jpg\n",
              "473  unknown/fab5b6a96ad9e75f48c8.jpg\n",
              "474  unknown/fb4f013fd7ee08a4a9bb.jpg\n",
              "475  unknown/fbecda8e838fcee4dd20.jpg\n",
              "476  unknown/fc1ba171d3a63345f516.jpg\n",
              "477  unknown/fd2807a7befa288f3321.jpg\n",
              "478  unknown/fd5737122300961d1bca.jpg\n",
              "479  unknown/fd79ce851d0af59df9c3.jpg\n",
              "480  unknown/fe8fcc2796872d12ff0d.jpg\n",
              "\n",
              "[481 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etnKhADoYQfK",
        "colab_type": "code",
        "outputId": "2483b07b-2ac4-466a-f90b-598ee3346815",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "model.predict(col[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-e8368582ea71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_41_input to have 4 dimensions, but got array with shape (600, 600, 3)"
          ]
        }
      ]
    }
  ]
}